{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Proof of BP4**\n",
        "\n",
        "BP3 relates the partial derivative of the cost function with respect to the biases in the network to the error term of the corresponding neuron. Specifically, it establishes that this derivative is equal to the error term itself:\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$$\n",
        "\n",
        "\n",
        "This equation simplifies how we can compute the gradient of the cost function concerning the biases, leveraging the computed error terms from the backpropagation algorithm.\n",
        "\n",
        "### **Derivation**\n",
        "\n",
        "To derive BP3, we start with the definition of the error term $\\delta_j^l$ for a neuron in layer $l$:\n",
        "\n",
        "$$\\delta^l_j = \\frac{\\partial C}{\\partial z^l_j}$$\n",
        "\n",
        "where $z_j^l$ is the weighted input to the neuron. Given that $z_j^l$ is directly affected by the bias $b_j^l$ (since $z_j^l = \\sum_k w_{jk}^l a_k^{l-1} + b_j^l$), the derivative of $z_j^l$ with respect to $b_j^l$ is 1:\n",
        "\n",
        "$$\\frac{\\partial z^l_j}{\\partial b^l_j} = 1$$\n",
        "\n",
        "Applying the chain rule, we find that:\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j} \\cdot \\frac{\\partial z^l_j}{\\partial b^l_j} = \\delta^l_j \\cdot 1 = \\delta^l_j$$\n",
        "\n",
        "This completes the proof that the gradient of the cost function concerning any bias in the network is equal to the error term for the corresponding neuron."
      ],
      "metadata": {
        "id": "7KP3pOS50X_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Proof of BP4**\n",
        "\n",
        "BP4 explains how to compute the gradient of the cost function concerning any weight in the network. It shows that this gradient can be expressed as the product of the error term of the neuron receiving the weight and the activation of the neuron providing the input:\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l$$\n",
        "\n",
        "This relationship is crucial for updating the weights in the network during the learning process.\n",
        "\n",
        "### **Derivation**\n",
        "\n",
        "Consider the effect of a weight $w_{jk}^l$ on the cost function $C$. This weight influences $C$ through its effect on the weighted input $z_j^l$, which then affects the activation $a_j^l$, and subsequently the overall cost $C$:\n",
        "\n",
        "$$z_j^l = \\sum_k w_{jk}^l a_k^{l-1} + b_j^l$$\n",
        "\n",
        "Given the activation function $\\sigma$, the activation $a_j^l$ is $\\sigma(z_j^l)$. The derivative of $z_j^l$ with respect to $w_{jk}^l$ isolates the activation $a_k^{l-1}$ from the previous layer:\n",
        "\n",
        "$$\\frac{\\partial z_j^l}{\\partial w_{jk}^l} = a_k^{l-1}$$\n",
        "\n",
        "\n",
        "Using the definition of the error term $\\delta_j^l = \\frac{\\partial C}{\\partial z_j^l}$, and applying the chain rule, we can express the partial derivative of $C$ with respect to $w_{jk}^l$ as:\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial w_{jk}^l} = \\frac{\\partial C}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial w_{jk}^l} = \\delta_j^l \\cdot a_k^{l-1}$$\n",
        "\n",
        "\n",
        "This establishes BP4, showing that the rate of change of the cost function concerning a weight is determined by the product of the error term for the neuron receiving that weight and the activation of the neuron providing the input."
      ],
      "metadata": {
        "id": "Ss__rxVr0bRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix-based approach to backpropagation over a mini-batch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Activation function and its derivative\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "# Loading and preprocessing the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# One-hot encode the targets\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Augment the input data with a bias term\n",
        "X_train_augmented = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
        "X_test_augmented = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
        "\n",
        "class MatrixBasedNN:\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for w in self.weights:\n",
        "            a = sigmoid(np.dot(a, w.T))\n",
        "        return a\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for w in self.weights:\n",
        "            z = np.dot(activation, w.T)\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = (activations[-1] - y) * sigmoid_prime(zs[-1])\n",
        "        nabla_w[-1] = np.dot(delta.T, activations[-2])\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(delta, self.weights[-l+1]) * sp\n",
        "            nabla_w[-l] = np.dot(delta.T, activations[-l-1])\n",
        "\n",
        "        return nabla_w\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "\n",
        "    def train(self, X, y, epochs, mini_batch_size, eta):\n",
        "        n = len(X)\n",
        "        for j in range(epochs):\n",
        "            permutation = np.random.permutation(n)\n",
        "            X_shuffled = X[permutation]\n",
        "            y_shuffled = y[permutation]\n",
        "            mini_batches = [(X_shuffled[k:k+mini_batch_size], y_shuffled[k:k+mini_batch_size])\n",
        "                            for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "\n",
        "# Initialize and train the network\n",
        "net = MatrixBasedNN([5, 3, 3])  # Including the bias input\n",
        "net.train(X_train_augmented, y_train, epochs=1000, mini_batch_size=10, eta=1.0)\n",
        "\n",
        "# Example: Evaluate the network\n",
        "predictions = net.feedforward(X_test_augmented)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "jAGps-tf0ibi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}